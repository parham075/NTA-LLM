{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Querying PDF With Astra and LangChain\n",
    "### A question-answering demo using Astra DB and LangChain, powered by Vector Search"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Pre-requisites:\n",
    "\n",
    "You need a **_Serverless Cassandra with Vector Search_** database on [Astra DB](https://astra.datastax.com) to run this demo. As outlined in more detail [here](https://docs.datastax.com/en/astra-serverless/docs/vector-search/quickstart.html#_prepare_for_using_your_vector_database), you should get a DB Token with role _Database Administrator_ and copy your Database ID: these connection parameters are needed momentarily.\n",
    "\n",
    "You also need an [OpenAI API Key](https://cassio.org/start_here/#llm-access) for this demo to work.\n",
    "\n",
    "#### What you will do:\n",
    "\n",
    "- Setup: import dependencies, provide secrets, create the LangChain vector store;\n",
    "- Run a Question-Answering loop retrieving the relevant headlines and having an LLM construct the answer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LangChain components to use\n",
    "from langchain.vectorstores.cassandra import Cassandra\n",
    "from langchain.indexes.vectorstore import VectorStoreIndexWrapper\n",
    "from langchain.llms import OpenAI\n",
    "from langchain.embeddings import OpenAIEmbeddings\n",
    "import yaml\n",
    "from langchain.text_splitter import CharacterTextSplitter\n",
    "# Support for dataset retrieval with Hugging Face\n",
    "from datasets import load_dataset\n",
    "\n",
    "# With CassIO, the engine powering the Astra DB integration in LangChain,\n",
    "# you will also initialize the DB connection:\n",
    "import cassio\n",
    "from PyPDF2 import PdfReader\n",
    "from typing_extensions import Concatenate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ASREA cridentials:\n",
    "You need to provide `ASTRA_DB_APPLICATION_TOKEN`, `ASTRA_DB_ID`, `OPENAI_API_KEY` in a yaml file under `/tmp/langchain-conf.yml` and populate the information you need as below:\n",
    "  ```python\n",
    "  OpenAI:\n",
    "    ASTRA_DB_APPLICATION_TOKEN: <ASTRA_DB_APPLICATION_TOKEN>\n",
    "    ASTRA_DB_ID: <ASTRA_DB_ID>\n",
    "    OPENAI_API_KEY: <OPENAI_API_KEY>\n",
    "    API_Endpoint: <API_Endpoint>\n",
    "  ```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specify the path to your YAML file\n",
    "OpenAI_conf = '/tmp/langchain-conf.yml'\n",
    "\n",
    "# Open the YAML file and load its contents\n",
    "with open(OpenAI_conf, 'r') as file:\n",
    "    config = yaml.safe_load(file)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "pdf_reader = PdfReader('./Attention Is All You Need.pdf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<PyPDF2._page._VirtualList at 0x7de7412d4a60>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pdf_reader.pages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "33800"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw_text = ''\n",
    "for i, page in enumerate(pdf_reader.pages):\n",
    "  if i<11: # analize the pdf until page 10\n",
    "    content = page.extract_text()\n",
    "    if content:\n",
    "      raw_text+= content\n",
    "len(raw_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initialize the connection to your database:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "cassio.init(token=config['OpenAI']['ASTRA_DB_APPLICATION_TOKEN'], database_id=config['OpenAI']['ASTRA_DB_ID'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create your LangChain vector store ... backed by Astra DB!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = OpenAI(openai_api_key=config['OpenAI']['OPENAI_API_KEY'])\n",
    "embedding = OpenAIEmbeddings(openai_api_key=config['OpenAI']['OPENAI_API_KEY'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "astra_vector_store = Cassandra(\n",
    "    embedding=embedding,\n",
    "    table_name=\"attention_all_you_need_demo\",\n",
    "    session=None,\n",
    "    keyspace=None,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We need to split the text using Character Text Split such that it sshould not increse token size\n",
    "text_splitter = CharacterTextSplitter(\n",
    "    separator = \"\\n\",\n",
    "    chunk_size = 800,\n",
    "    chunk_overlap  = 200,\n",
    "    length_function = len,\n",
    ")\n",
    "texts = text_splitter.split_text(raw_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load the dataset into the vector store\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "astra_vector_store.add_texts(texts)\n",
    "\n",
    "print(\"Inserted %i headlines.\" % len(texts))\n",
    "\n",
    "astra_vector_index = VectorStoreIndexWrapper(vectorstore=astra_vector_store)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run the QA cycle\n",
    "\n",
    "Simply run the cells and ask a question -- or `quit` to stop. (you can also stop execution with the \"â–ª\" button on the top toolbar)\n",
    "\n",
    "Here are some suggested questions:\n",
    "- Please explain attention for a child?_\n",
    "- How many encoders and decoders used in the article?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "first_question = True\n",
    "while True:\n",
    "    if first_question:\n",
    "        query_text = input(\"\\nEnter your question (or type 'quit' to exit): \").strip()\n",
    "    else:\n",
    "        query_text = input(\"\\nWhat's your next question (or type 'quit' to exit): \").strip()\n",
    "\n",
    "    if query_text.lower() == \"quit\":\n",
    "        break\n",
    "\n",
    "    if query_text == \"\":\n",
    "        continue\n",
    "\n",
    "    first_question = False\n",
    "\n",
    "    print(\"\\nQUESTION: \\\"%s\\\"\" % query_text)\n",
    "    answer = astra_vector_index.query(query_text, llm=llm).strip()\n",
    "    print(\"ANSWER: \\\"%s\\\"\\n\" % answer)\n",
    "\n",
    "    print(\"FIRST DOCUMENTS BY RELEVANCE:\")\n",
    "    for doc, score in astra_vector_store.similarity_search_with_score(query_text, k=4):\n",
    "        print(\"    [%0.4f] \\\"%s ...\\\"\" % (score, doc.page_content[:84]))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
